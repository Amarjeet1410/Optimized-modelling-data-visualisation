{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fWkXj36F2glh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\textbf{1.}$ What is the minimizer and minimum function value of f(x) and g(x) ? Are both the function convex ? What\n",
        "is a suitable initial choice of B (denoted by B0, i.e. Replacement of first ?? in the Algorithm 3)? Justify\n",
        "with proper reasons.\n"
      ],
      "metadata": {
        "id": "AIw73uTcLTIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "For $f(x)$:\n",
        "$\n",
        "\\frac{\\partial f}{\\partial x_i} = 8(x_i - 1) + 8(x_{2i}^2 - x_{i+1}) = 0\n",
        "$\n",
        "\n",
        "Substitute $x_i = 1$:\n",
        "$\n",
        "\\frac{\\partial f}{\\partial x_i} \\Big|_{x_i=1} = 8(1 - 1) + 8(x_{2i}^2 - x_{i+1}) = 0\n",
        "$\n",
        "\n",
        "This simplifies to $-8(x_{2i}^2 - x_{i+1}) = 0$. Since $x_{2i}^2$ is always non-negative, the only solution is $x_{i+1} = 1$.\n",
        "\n",
        "For $g(x)$:\n",
        "$\n",
        "\\frac{\\partial g}{\\partial x_i} = 2(x_1 - x_{2i}) - 2(x_{2i-1} - x_i) + 2(x_i - 1) = 0\n",
        "$\n",
        "\n",
        "Substitute $x_i = 1$:\n",
        "$\n",
        "\\frac{\\partial g}{\\partial x_i} \\Big|_{x_i=1} = 2(x_1 - x_{2i}) - 2(x_{2i-1} - 1) + 2(1 - 1) = 0\n",
        "$\n",
        "\n",
        "This simplifies to $2(x_1 - x_{2i}) - 2(x_{2i-1} - 1) = 0$. Similarly, since $x_{2i-1} \\geq 1$ and $x_{2i} \\geq 1$, the only solution is $x_1 = 1$.\n",
        "\n",
        "Therefore, $x_i = 1$ for all $i$ is indeed a solution to both systems of equations, confirming that $x_i = 1$ is a minimizer for both $f(x)$ and $g(x)$.\n",
        "\n",
        "For $f(x)$:\n",
        "$f(x) = \\sum_{i=1}^{n-1} [4(x_{2i} - x_{i+1})^2 + (x_i - 1)^2]$\n",
        "\n",
        "Substitute $x_i = 1$:\n",
        "$f(x) = \\sum_{i=1}^{n-1} [4(1 - x_{i+1})^2 + (1 - 1)^2]$\n",
        "\n",
        "Simplifying further:\n",
        "$f(x) = 4\\sum_{i=1}^{n-1} (1 - x_{i+1})^2$\n",
        "\n",
        "Since $x_{i+1} = 1$ for all $i$, each term in the sum becomes zero, and the minimum value of $f(x)$ is $0$.\n",
        "\n",
        "For $g(x)$:\n",
        "$g(x) = \\sum_{i=1}^{n} [(x_1 - x_{2i})^2 + (x_i - 1)^2]$\n",
        "\n",
        "Substitute $x_i = 1$:\n",
        "$g(x) = \\sum_{i=1}^{n} [(1 - x_{2i})^2 + (1 - 1)^2]$\n",
        "\n",
        "Simplifying further:\n",
        "$g(x) = \\sum_{i=1}^{n} (1 - x_{2i})^2$\n",
        "\n",
        "Since $x_{2i} = 1$ for all $i$, each term in the sum becomes zero, and the minimum value of $g(x)$ is $0$.\n",
        "\n",
        "Therefore, when $x_i = 1$ for all $i$, both $f(x)$ and $g(x)$ achieve their minimum values of $0$.\n",
        "\n",
        "**Initial Choice of $B_0$:**\n",
        " a common and often effective choice for the initial Hessian approximation ($B_0$) is the identity matrix ($I$).\n",
        "\n",
        "**Justification:**\n",
        "1. **Positivity Definite:** The identity matrix is always positive definite.\n",
        "2. **Simplicity:** Choosing $B_0 = I$ is computationally simple.\n",
        "3. **General Applicability:** It is a general-purpose choice that often performs well across different problems.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YKnJwMspbFoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# Define the functions f(x) and g(x)\n",
        "def f(x):\n",
        "    product_term = np.prod([4 * (x[i]**2 - x[i+1])**2 + (x[i] - 1)**2 for i in range(len(x)-1)])\n",
        "    return product_term\n",
        "\n",
        "def g(x):\n",
        "    sum_term = np.sum([(x[0] - x[i]**2)**2 + (x[i] - 1)**2 for i in range(len(x))])\n",
        "    return sum_term\n",
        "\n",
        "# Algorithm 3: BFGS Algorithm\n",
        "def bfgs_algorithm(x0, tolerance):\n",
        "    B0 = np.eye(len(x0))  # Initial choice of B_0 as the identity matrix\n",
        "    k = 0\n",
        "\n",
        "    while np.linalg.norm(grad_f(x0)) > tolerance:\n",
        "        pk = -np.dot(B0, grad_f(x0))\n",
        "        alpha_k = minimize(lambda alpha: f(x0 + alpha * pk), 0).x[0]\n",
        "        x0_new = x0 + alpha_k * pk\n",
        "        sk = x0_new - x0\n",
        "        yk = grad_f(x0_new) - grad_f(x0)\n",
        "\n",
        "        # BFGS update formula for Hessian approximation\n",
        "        B0 = B0 + np.outer(yk, yk) / np.dot(yk, sk) - np.dot(B0, np.outer(sk, sk)).dot(B0) / np.dot(sk, B0.dot(sk))\n",
        "\n",
        "        x0 = x0_new\n",
        "        k += 1\n",
        "\n",
        "    return x0, f(x0), k\n",
        "\n",
        "# Gradient of f(x)\n",
        "def grad_f(x):\n",
        "    gradient = np.zeros_like(x)\n",
        "    for i in range(len(x)-1):\n",
        "        gradient[i] = 4 * (x[i]**2 - x[i+1]) * (8 * x[i]**2 - 4 * x[i+1] + 2) + 2 * (x[i] - 1)\n",
        "    gradient[-1] = 2 * (x[-1] - 1)\n",
        "    return gradient\n",
        "\n",
        "# Initial guess\n",
        "initial_guess = np.ones(5)\n",
        "\n",
        "# Set tolerance for stopping criterion\n",
        "tolerance = 1e-6\n",
        "\n",
        "# Run BFGS algorithm for function f(x)\n",
        "result_f = bfgs_algorithm(initial_guess, tolerance)\n",
        "\n",
        "# Run BFGS algorithm for function g(x)\n",
        "result_g = bfgs_algorithm(initial_guess, tolerance)\n",
        "\n",
        "# Display results\n",
        "print(\"Results for f(x):\")\n",
        "print(\"Minimizer:\", result_f[0])\n",
        "print(\"Minimum function value:\", result_f[1])\n",
        "print(\"Number of iterations:\", result_f[2])\n",
        "\n",
        "print(\"\\nResults for g(x):\")\n",
        "print(\"Minimizer:\", result_g[0])\n",
        "print(\"Minimum function value:\", result_g[1])\n",
        "print(\"Number of iterations:\", result_g[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7NGtD-YI9X_",
        "outputId": "5a37bc2f-2ee3-4c49-ed0e-e63d613b976d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for f(x):\n",
            "Minimizer: [1. 1. 1. 1. 1.]\n",
            "Minimum function value: 0.0\n",
            "Number of iterations: 0\n",
            "\n",
            "Results for g(x):\n",
            "Minimizer: [1. 1. 1. 1. 1.]\n",
            "Minimum function value: 0.0\n",
            "Number of iterations: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\textbf{Question 2.}$"
      ],
      "metadata": {
        "id": "ueZt1mrbNDCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# Define the objective function f(x)\n",
        "def f(x):\n",
        "    return np.sum(4 * (x[:-1]**2 - x[1:])**2 + (x[:-1] - 1)**2)\n",
        "\n",
        "# Gradient of f(x)\n",
        "def gradient_f(x):\n",
        "    n = len(x)\n",
        "    grad = np.zeros_like(x)\n",
        "    grad[:-1] += 8 * (x[:-1]**2 - x[1:]) * (2 * x[:-1])\n",
        "    grad[1:] += -8 * (x[:-1]**2 - x[1:]) + 2 * (x[:-1] - 1)\n",
        "    return grad\n",
        "\n",
        "# BFGS optimization algorithm with more efficient line search\n",
        "def bfgs_with_line_search(func, grad_func, x0):\n",
        "    start_time = time.time()\n",
        "\n",
        "    result = minimize(func, x0, jac=grad_func, method='L-BFGS-B', options={'maxiter': 100})\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    return result.x, result.fun, elapsed_time\n",
        "\n",
        "# Test for different values of n\n",
        "dimensions = [1000, 2500, 5000, 7500, 10000]\n",
        "results = []\n",
        "\n",
        "# Display header for the results table\n",
        "print(\"{:<10} {:<20} {:<20}\".format(\"Dimension\", \"Minimum Value\", \"Time Taken (s)\"))\n",
        "print(\"=\"*50)\n",
        "\n",
        "for n in dimensions:\n",
        "    x0 = np.zeros(n)\n",
        "    minimizer, min_value, elapsed_time = bfgs_with_line_search(f, gradient_f, x0)\n",
        "    results.append((n, min_value, elapsed_time))\n",
        "    print(\"{:<10} {:<20} {:<20}\".format(n, min_value, elapsed_time))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sbBAUM6I9Vf",
        "outputId": "73e2d2d9-497a-43c4-cc1b-683fce42e4c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimension  Minimum Value        Time Taken (s)      \n",
            "==================================================\n",
            "1000       3.886048115018858    0.01813983917236328 \n",
            "2500       3.8866667640074484   0.012986421585083008\n",
            "5000       3.8890669551610544   0.024234771728515625\n",
            "7500       3.8886473663915497   0.12154912948608398 \n",
            "10000      3.888699500343477    0.03904128074645996 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\textbf{Question 3.}$"
      ],
      "metadata": {
        "id": "u7UWdKMUNMZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# Define the objective function g(x)\n",
        "def g(x):\n",
        "    return np.sum((x[0] - x[1:]**2)**2 + (x[1:] - 1)**2)\n",
        "\n",
        "# Gradient of g(x)\n",
        "def gradient_g(x):\n",
        "    n = len(x)\n",
        "    grad = np.zeros_like(x)\n",
        "    grad[0] = 2 * (x[0] - x[1]**2)\n",
        "    grad[1:] += 2 * (x[1:] - 1) * (-2 * x[1:])\n",
        "    grad[0] += 2 * (x[0] - x[1:]**2) @ (-2 * x[1:])\n",
        "    return grad\n",
        "\n",
        "# BFGS optimization algorithm with more efficient line search\n",
        "def bfgs_with_line_search(func, grad_func, x0):\n",
        "    start_time = time.time()\n",
        "\n",
        "    result = minimize(func, x0, jac=grad_func, method='L-BFGS-B', options={'maxiter': 100})\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    return result.x, result.fun, elapsed_time\n",
        "\n",
        "# Test for different values of n\n",
        "dimensions = [1000, 2500, 5000, 7500, 10000]\n",
        "results = []\n",
        "\n",
        "# Display header for the results table\n",
        "print(\"{:<10} {:<20} {:<20}\".format(\"Dimension\", \"Minimum Value\", \"Time Taken (s)\"))\n",
        "print(\"=\"*50)\n",
        "\n",
        "for n in dimensions:\n",
        "    x0 = np.zeros(n)\n",
        "    minimizer, min_value, elapsed_time = bfgs_with_line_search(g, gradient_g, x0)\n",
        "    results.append((n, min_value, elapsed_time))\n",
        "    print(\"{:<10} {:<20} {:<20}\".format(n, min_value, elapsed_time))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkybaAAuKLfo",
        "outputId": "0ffd7a12-5a09-48b1-8307-310e4d3d52c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimension  Minimum Value        Time Taken (s)      \n",
            "==================================================\n",
            "1000       999.0                0.003514528274536133\n",
            "2500       2499.0               0.004128932952880859\n",
            "5000       4999.0               0.007193088531494141\n",
            "7500       7499.0               0.010080575942993164\n",
            "10000      9999.0               0.013151884078979492\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\textbf{Question 4.}$"
      ],
      "metadata": {
        "id": "psPZhe9GPZ_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def objective_function(x):\n",
        "    return np.prod([4*(x[i]**2 - x[i+1])**2 + (x[i] - 1)**2 for i in range(len(x)-1)])\n",
        "\n",
        "def gradient(x):\n",
        "    n = len(x)\n",
        "    grad = np.zeros(n)\n",
        "    for i in range(n-1):\n",
        "        grad[i] = -8 * (x[i]**2 - x[i+1]) * (2*x[i] - 2) + 2 * (x[i] - 1)\n",
        "        grad[i+1] = 8 * (x[i]**2 - x[i+1])  # Special case for the last element\n",
        "    return grad\n",
        "\n",
        "def hessian(x):\n",
        "    n = len(x)\n",
        "    hess = np.zeros((n, n))\n",
        "    for i in range(n-1):\n",
        "        hess[i, i] = 8 * (x[i]**2 - x[i+1]) + 16 * x[i]**2 - 8\n",
        "        hess[i, i+1] = -16 * x[i]\n",
        "        hess[i+1, i] = -16 * x[i]  # Symmetric\n",
        "        hess[i+1, i+1] = 8 * (x[i]**2 - x[i+1])  # Special case for the last element\n",
        "    return hess\n",
        "\n",
        "def preconditioned_gradient(x, preconditioner):\n",
        "    return preconditioner @ gradient(x)\n",
        "\n",
        "def preconditioned_hessian(x, preconditioner):\n",
        "    return preconditioner @ hessian(x) @ preconditioner.T\n",
        "\n",
        "def backtracking_line_search(x, gradient, direction, alpha0, rho, gamma):\n",
        "    alpha = alpha0\n",
        "    while objective_function(x + alpha * direction) > objective_function(x) + gamma * alpha * np.dot(gradient, direction):\n",
        "        alpha = rho * alpha\n",
        "    return alpha\n",
        "\n",
        "def newtons_method_with_preconditioning(x0, tolerance, preconditioner):\n",
        "    x = x0.copy()\n",
        "    k = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    while np.linalg.norm(gradient(x)) > tolerance:\n",
        "        precond_gradient = preconditioned_gradient(x, preconditioner)\n",
        "        precond_hessian = preconditioned_hessian(x, preconditioner)\n",
        "\n",
        "        direction = -np.linalg.solve(precond_hessian, precond_gradient)\n",
        "        step_size = backtracking_line_search(x, gradient(x), direction, alpha0=0.9, rho=0.5, gamma=0.5)\n",
        "        x = x + step_size * direction\n",
        "        k += 1\n",
        "\n",
        "    end_time = time.time()\n",
        "    time_taken = end_time - start_time\n",
        "\n",
        "    return x, k, time_taken\n",
        "\n",
        "# Test for different values of n\n",
        "n_values = [1000, 2500, 5000, 7500, 10000]\n",
        "tolerance = 1e-6  # Stopping tolerance\n",
        "preconditioner = preconditioner + regularization_constant * np.eye(len(preconditioner))\n",
        "# Identity matrix as a simple preconditioner\n",
        "\n",
        "# Display results in a table\n",
        "print(\"{:<10} {:<25} {:<15}\".format(\"n\", \"Minimizer\", \"Time Taken (s)\"))\n",
        "for n in n_values:\n",
        "    x0 = np.zeros(n)\n",
        "\n",
        "    start_time = time.time()\n",
        "    result = newtons_method_with_preconditioning(x0, tolerance, preconditioner[:n, :n])\n",
        "    end_time = time.time()\n",
        "\n",
        "    minimizer, iterations, time_taken = result\n",
        "    print(\"{:<10} {:<25} {:<15}\".format(n, str(minimizer), time_taken))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "N7cukyBUCkHX",
        "outputId": "2f326fa7-71f0-4f89-c02e-e6f87b3d80cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'preconditioner' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-84ddf14fe540>\u001b[0m in \u001b[0;36m<cell line: 59>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mn_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-6\u001b[0m  \u001b[0;31m# Stopping tolerance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mpreconditioner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreconditioner\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mregularization_constant\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreconditioner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;31m# Identity matrix as a simple preconditioner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'preconditioner' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DJjSZj05KdUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a66NMX0UKdJl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}